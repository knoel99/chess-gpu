# â™Ÿ Chess-GPU

Apprendre Ã  jouer aux Ã©checs par imitation des Grands MaÃ®tres, avec des rÃ©seaux de neurones entraÃ®nÃ©s sur GPU.

## Structure

```
src/
â”œâ”€â”€ common/              # code partagÃ© (tÃ©lÃ©chargement, prÃ©paration)
â”œâ”€â”€ phase1_mlp/          # rÃ©seau feedforward MLP
â””â”€â”€ phase2_transformer/  # Transformer avec attention (Ã  venir)

results/
â””â”€â”€ phase1_mlp/          # rÃ©sultats et article (results.md)

docs/                    # documentation thÃ©orique (matrices, modÃ¨le)
```

## Phase 1 â€” MLP (feedforward)

RÃ©seau dense 832â†’1024â†’512â†’256â†’N entraÃ®nÃ© sur 208k parties de 10 GMs.
- **Top-1 accuracy** : 24.8% â€” **Top-5** : 52.5%
- **Elo estimÃ©** : ~838â€“950 vs Stockfish 1350
- ğŸ“„ [RÃ©sultats dÃ©taillÃ©s](results/phase1_mlp/results.md)

## Phase 2 â€” Transformer

Architecture avec mÃ©canisme d'attention pour capturer le contexte des coups prÃ©cÃ©dents.

- **Encodage enrichi** : 846 features (piÃ¨ces + tour + roque + en-passant + nÂ° coup)
- **SÃ©quence** : 16 derniÃ¨res positions (8 coups complets de contexte)
- **Architecture** : 4 couches Ã— 8 tÃªtes d'attention, d_model=256, ffn=1024
- **Masque causal** : chaque position ne voit que les prÃ©cÃ©dentes

```bash
cd /content && python chess-gpu/run_colab.py --phase2   # phase 2 seule
cd /content && python chess-gpu/run_colab.py --all      # phases 1 + 2
```

## DÃ©marrage rapide (Colab)

```bash
cd /content && python chess-gpu/run_colab.py            # phase 1 (MLP)
cd /content && python chess-gpu/run_colab.py --phase2   # phase 2 (Transformer)
cd /content && python chess-gpu/run_colab.py --all      # les deux
```

## Licence

MIT